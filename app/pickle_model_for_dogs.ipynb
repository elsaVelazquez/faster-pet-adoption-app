{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' heavily borrowed from https://github.com/GalvanizeDataScience/lectures/blob/Denver/text-classification/frank-burkholder/naive_bayes_sklearn.ipynb'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' heavily borrowed from https://github.com/GalvanizeDataScience/lectures/blob/Denver/text-classification/frank-burkholder/naive_bayes_sklearn.ipynb'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "required field \"type_ignores\" missing from Module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/codeop.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, filename, symbol)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mcodeob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcodeob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_flags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: required field \"type_ignores\" missing from Module"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['adoptable', 'adopted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_csv = '../data/csv/fullDB_valid_csv.csv'\n",
    "df1 = pd.read_csv(path_csv)\n",
    "\n",
    "#impute empty records\n",
    "df = df1.fillna(\"None\")\n",
    "\n",
    "#not necessary cleaning\n",
    "df.drop(['photos', 'videos', 'distance', 'status_changed_at', 'published_at', 'distance', 'contact', 'organization_animal_id', 'type', 'photos'], axis = 1, inplace= True)\n",
    "\n",
    "df = pd.get_dummies(df, columns=['status'])\n",
    "df.drop(\"status_adoptable\", axis = 1, inplace=True)\n",
    "# print(df.shape)\n",
    "\n",
    "df_content = df[[\"status_adopted\", \"description\"]].copy()\n",
    "# print(df_content.head())\n",
    "\n",
    "X = df_content[\"description\"] #data\n",
    "X.to_numpy()\n",
    "# print(X)\n",
    "\n",
    "y = df_content[\"status_adopted\"] #target\n",
    "y.to_numpy()\n",
    "# print(y)  \n",
    "# 0 == negative,  1 == positive                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meet Ruff, Ruff is a smaller breed - probably around 10 lbs at  6-7  months old and a complete mix...\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722\n",
      "722\n"
     ]
    }
   ],
   "source": [
    "# print(df_content.head())\n",
    "# print(X)\n",
    "# print(y)\n",
    "print(len(X)) #722\n",
    "print(len(y)) #722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_stopwords = ['old', 'mix', 'dogs', '039', 'amp', 'sweet', 'year', 'years', 'website' , 'loves', 'adoption', 'application', 'shelter', 'rescue', 'rescued']\n",
    "new_stopwords_list = stop_words.union(new_stopwords)\n",
    "\n",
    "count_vect = CountVectorizer(lowercase=True, tokenizer=None, stop_words = new_stopwords_list,  analyzer='word', max_df=1.0, min_df=1,  max_features=None) \n",
    "                             \n",
    "count_vect.fit(X)\n",
    "\n",
    "target_names = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None,\n",
      "                stop_words={'039', 'a', 'about', 'above', 'adoption', 'after',\n",
      "                            'again', 'against', 'ain', 'all', 'am', 'amp', 'an',\n",
      "                            'and', 'any', 'application', 'are', 'aren',\n",
      "                            \"aren't\", 'as', 'at', 'be', 'because', 'been',\n",
      "                            'before', 'being', 'below', 'between', 'both',\n",
      "                            'but', ...},\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of X_counts is <class 'scipy.sparse.csr.csr_matrix'>.\n",
      "The X matrix has 722 rows (documents) and 1806 columns (words).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_counts = count_vect.transform(X)\n",
    "print(\"The type of X_counts is {0}.\".format(type(X_counts)))\n",
    "print(\"The X matrix has {0} rows (documents) and {1} columns (words).\".format(\n",
    "        X_counts.shape[0], X_counts.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "tfidf_transformer.fit(X_counts)\n",
    "X_tfidf = tfidf_transformer.transform(X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577, 1806)\n",
      "(577,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 1806)\n",
      "(145,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "nb_model.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: 0, name: 1\n",
      "Top 20 tokens:  ['none', 'good', 'dog', 'girl', 'meet', 'female', 'pit', 'foster', 'name', 'pounds', 'please', 'friendly', 'cats', 'boy', 'around', 'weight', 'fill', 'breed', 'spayed', 'playful']\n",
      "\n",
      "Target: 1, name: 1\n",
      "Top 20 tokens:  ['online', 'none', 'fill', 'official', 'meet', 'home', 'looking', 'dog', 'forever', 'puppy', 'name', 'lbs', 'male', 'please', 'hi', '20', 'came', 'months', 'shepherd', 'month']\n"
     ]
    }
   ],
   "source": [
    "feature_words = count_vect.get_feature_names()\n",
    "n = 20 #number of top words associated with the category that we wish to see\n",
    "\n",
    "for cat in range(len(categories)): #categories == ['adoptable', 'adopted']\n",
    "    print(f\"\\nTarget: {cat}, name: {target_names[cat]}\")\n",
    "    log_prob = nb_model.feature_log_prob_[cat]\n",
    "    i_topn = np.argsort(log_prob)[::-1][:n]\n",
    "    features_topn = [feature_words[i] for i in i_topn]\n",
    "    print(f\"Top {n} tokens: \", features_topn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: 0, name: 1\n",
      "Top 10 tokens:  ['none', 'good', 'dog', 'girl', 'meet', 'female', 'pit', 'foster', 'name', 'pounds'] [1132  730  548  720 1038  649 1242  670 1099 1273]\n",
      "\n",
      "Target: 1, name: 1\n",
      "Top 10 tokens:  ['online', 'none', 'fill', 'official', 'meet', 'home', 'looking', 'dog', 'forever', 'puppy'] [1162 1132  653 1148 1038  793  976  548  666 1307]\n"
     ]
    }
   ],
   "source": [
    "feature_words = count_vect.get_feature_names()\n",
    "# print(feature_words)\n",
    "n = 10 #number of top words associated with the category that we wish to see\n",
    "feat_val_dict = {}\n",
    "\n",
    "for cat in range(len(categories)): #categories == ['adoptable', 'adopted']\n",
    "    # print(cat)\n",
    "    print(f\"\\nTarget: {cat}, name: {target_names[cat]}\")\n",
    "    log_prob = nb_model.feature_log_prob_[cat]\n",
    "    # print(nb_model.feature_log_prob_[cat])\n",
    "    i_topn = np.argsort(log_prob)[::-1][:n]\n",
    "    features_topn = [feature_words[i] for i in i_topn]\n",
    "    print(f\"Top {n} tokens: \", features_topn, i_topn)\n",
    "    # for word in features_topn:\n",
    "    #     print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = nb_model.predict(X_test)\n",
    "# X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8547008547008548"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71216066, 0.75930515, 0.86844199, 0.90706522, 0.82932451,\n",
       "       0.87140013, 0.67993514, 0.97125573, 0.57241605, 0.7893452 ,\n",
       "       0.72058565, 0.8193681 , 0.64633598, 0.64088113, 0.78631544,\n",
       "       0.59910174, 0.72793925, 0.42637203, 0.74771564, 0.97826375,\n",
       "       0.87621175, 0.83847934, 0.42637203, 0.97125573, 0.82459837,\n",
       "       0.5622289 , 0.75365443, 0.88270828, 0.74722437, 0.78644864,\n",
       "       0.66062795, 0.91917311, 0.92825488, 0.42637203, 0.71207837,\n",
       "       0.63259685, 0.42637203, 0.64736833, 0.81956706, 0.79830218,\n",
       "       0.81997776, 0.8271789 , 0.84080842, 0.97395362, 0.42637203,\n",
       "       0.77029561, 0.97125573, 0.83654551, 0.60073157, 0.72273535,\n",
       "       0.6704058 , 0.67639054, 0.69067067, 0.7889291 , 0.90553464,\n",
       "       0.90292565, 0.29503247, 0.69614049, 0.83462137, 0.78973342,\n",
       "       0.72599417, 0.63641401, 0.72587675, 0.83836189, 0.67036371,\n",
       "       0.70725115, 0.77720308, 0.64089543, 0.85864731, 0.63837743,\n",
       "       0.61157425, 0.90212459, 0.88829324, 0.6133927 , 0.65052987,\n",
       "       0.81745308, 0.80162315, 0.22030363, 0.42637203, 0.81674397,\n",
       "       0.42637203, 0.73839178, 0.6376127 , 0.42637203, 0.92344362,\n",
       "       0.90527669, 0.42637203, 0.8992005 , 0.92077531, 0.70648877,\n",
       "       0.70362171, 0.76816128, 0.63311403, 0.87093828, 0.74422609,\n",
       "       0.76183741, 0.90527669, 0.74704012, 0.72591481, 0.77681422,\n",
       "       0.84120609, 0.91444514, 0.71056325, 0.97125573, 0.59169443,\n",
       "       0.93548418, 0.66620228, 0.63297402, 0.8992005 , 0.88322925,\n",
       "       0.74413642, 0.73844432, 0.56915171, 0.75193224, 0.42637203,\n",
       "       0.78510344, 0.42637203, 0.8399955 , 0.54884365, 0.7546903 ,\n",
       "       0.92243947, 0.94353624, 0.80346617, 0.70852966, 0.84145424,\n",
       "       0.80264145, 0.42637203, 0.69031656, 0.74486099, 0.86058211,\n",
       "       0.68804626, 0.91050661, 0.70077516, 0.74027785, 0.75193224,\n",
       "       0.79917381, 0.64814543, 0.8053569 , 0.93374952, 0.74219512,\n",
       "       0.58802189, 0.79399843, 0.70362171, 0.68886456, 0.82726411])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = nb_model.predict_proba(X_test)[:,1]\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh=0.7\n",
    "y_pred = (y_pred_proba>=thresh).astype(int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7783251231527093"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps:\n",
    "# 1. pickle and save the model \n",
    "import pickle\n",
    "save_documents = open(\"pickled_algos/pickled_nb.pickle\",\"wb\")\n",
    "pickle.dump(nb_model, save_documents)\n",
    "save_documents.close()\n",
    "\n",
    "tfidf = tfidf_transformer\n",
    "save_documents = open(\"pickled_algos/tfidf_transformer.pickle\", \"wb\")\n",
    "pickle.dump(tfidf, save_documents)\n",
    "save_documents.close()\n",
    "\n",
    "count_vect = count_vect\n",
    "save_documents = open(\"pickled_algos/count_vect.pickle\", \"wb\")\n",
    "pickle.dump(count_vect, save_documents)\n",
    "save_documents.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "# 2. test the model by \n",
    "#turn this whoel cell into a function that takes in a string\n",
    "#predict_one\n",
    "with open('pickled_algos/pickled_nb.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open('pickled_algos/tfidf_transformer.pickle', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "with open('pickled_algos/count_vect.pickle', 'rb') as f:\n",
    "    cv = pickle.load(f)\n",
    "\n",
    "# string_pred = ['this girl is a foster pit and has none of her teeth']  \n",
    "string_pred = ['fill out an online form to find this male puppy a forever home']\n",
    "\n",
    "cv_transformed = cv.transform(string_pred) #counts how many words\n",
    "tfidf_transformed = tfidf.transform(cv_transformed)  #tf == cv . \n",
    "string_predicted = model.predict(tfidf_transformed) \n",
    "res = str(string_predicted[0])\n",
    "if res == '0':\n",
    "    print('Negative Sentiment')\n",
    "else:\n",
    "    print(\"Positive Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive Sentiment'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_one(input):\n",
    "    with open('pickled_algos/pickled_nb.pickle', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    with open('pickled_algos/tfidf_transformer.pickle', 'rb') as f:\n",
    "        tfidf = pickle.load(f)\n",
    "\n",
    "    with open('pickled_algos/count_vect.pickle', 'rb') as f:\n",
    "        cv = pickle.load(f)\n",
    "\n",
    "  \n",
    "    cv_transformed = cv.transform(string_pred) #counts how many words\n",
    "    tfidf_transformed = tfidf.transform(cv_transformed)  #tf == cv . \n",
    "    string_predicted = model.predict(tfidf_transformed) \n",
    "    res = str(string_predicted[0])\n",
    "    if res == '0':\n",
    "        res = ('Negative Sentiment')\n",
    "    else:\n",
    "        res = (\"Positive Sentiment\")\n",
    "    return res\n",
    "\n",
    "# string_pred = ['this girl is a foster pit and has none of her teeth']\n",
    "string_pred = ['fill out an online form to find this male puppy a forever home']\n",
    "\n",
    "predict_one(string_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
